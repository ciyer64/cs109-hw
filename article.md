**OP-ED: OF POLITICS AND PREDICTABILITY**
Curren A. Iyer
Nov 10, 2014

Probability is everywhere.  From everyday tasks like checking the weather, to more addictive habits like gambling, our lives involve some degree of uncertainty, and thus we are forced to rely upon chance.  Politics is no exception.  When discussing major upcoming events, such as elections, we love to make some prediction about what will happen.  As we are a democracy (in some cases), winning elections depends on getting the votes of the people.  Therefore it's only natural that we look at one of the basic metrics for hearing the public's opinion: polls.

But how accurate are these polls?  Often times people are skeptical of what polls say, as there are numerous factors that can potentially skew the results, such as who is responding to these polls and how opinions change over time.  From here on we will refer to these factors *biases*.  I will make the case to you, the reader, that polls are in fact very accurate in predicting election results; we just have to be diligent in the way that we interpret the data, for it is much more complex than what is presented at face value.

Let us look at the 2014 elections, where, against all odds, the Republicans won 23 seats (quite a feat!).  In the aftermath of this historic victory, major political buffs and the media were scratching their heads: how could this have happened?  Interestingly enough, however, these results were not that startling.  First, let's take a sample of polls from the time period and take the average across their predicted spread (where *spread* refers to the difference in percentage of votes for the Republican vs. Democratic candidate).  If the spread for a state favors a certain party, then we can say that that party is expected to win that state's seat (i.e. if the spread is 15% in favor of the Republicans, they would be expected to win that seat).  After taking the average for the expected spread among various polls across the nation, let us compare that to the actual results of the election.  We can see our model in the figure below. 

![image](images/Senate_Seats.png?raw=true)
Caption: Our results compared to the actual number of seats won (23) - What went wrong?

![image](images/Senate_Seats_Weighted.png?raw=true)
Caption: Our results after giving different polls more weight, compared to the actual number of seats won (23).  Close, but can we do better?

![image](images/Senate_Seats_Weighted_Unbiased.png?raw=true)
Caption: Our results after giving different polls more weight and removing a bias factor, compared to the actual number of seats won (23).  Not bad!